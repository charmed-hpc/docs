---
relatedlinks: "[Slurm&#32Workload&#32Manager&#32-&#32Quick&#32Start&#32Administrator&#32Guide&#32-&#32High&#32Availability](https://slurm.schedmd.com/quickstart_admin.html#HA), [Slurm&#32Workload&#32Manager&#32-&#32Quick&#32Start&#32Administrator&#32Guide&#32-&#32Configuration](https://slurm.schedmd.com/quickstart_admin.html#Config), [Slurm&#32Workload&#32Manager&#32-&#32slurm.conf&#32-&#32SlurmctldHost](https://slurm.schedmd.com/slurm.conf.html#OPT_SlurmctldHost), [Ubuntu&#32High-Performance&#32Computing&#32Spec:&#32slurmctld&#32high-availability&#32implementation&#32in&#32Charmed&#32HPC](https://hackmd.io/@ubuntu-hpc/HkqyL5K4le)"
---

(explanation-high-availability)=
# High Availability

High availability (HA) refers to the ability of a system to continue functioning despite component failures. A motivation for HA being to eliminate single points of failure and minimize system downtime. HA can be provided through component redundancy, such as a computer cluster with multiple controllers that continues to function even if a controller has failed.

When an HA cluster distributes incoming requests across multiple controllers simultaneously, it is referred to as an active-active setup. In contrast, an active-passive setup designates one controller as the primary, which handles all requests while the others remain on standby, ready for one to take over should the primary fail.

(explanation-slurmctld-high-availability)=
## Slurmctld high availability

The `slurmctld` controller service supports HA through an active-passive setup. The primary controller serves all Slurm client requests while backup controllers wait in standby. Controllers are defined by `SlurmctldHost` entries in the _slurm.conf_ configuration file, with the first entry being the primary and all others being backups, with backup fail-over order being the order defined in the file.

Charmed HPC uses this functionality to allow scaling of a deployed `slurmctld` charm.

(explanation-slurmctld-high-availability-state-save-location)=
### Shared `StateSaveLocation` using `filesystem-client` charm

For `slurmctld` HA to function, all `slurmctld` controllers require mounting of the same shared file system to provide a common [`StateSaveLocation`](https://slurm.schedmd.com/slurm.conf.html#OPT_StateSaveLocation) directory to hold controller state data. This directory governs the responsiveness and throughput of the cluster, so it should be hosted on a file system with low latency. It is therefore recommended that the file system **not be the same as the file system used for the cluster compute nodes** to avoid I/O-intensive user jobs from impacting `slurmctld` responsiveness.

To allow for flexibility in shared file system for the `StateSaveLocation`, Charmed HPC implements support for the [`filesystem-client` charm](https://github.com/charmed-hpc/filesystem-charms) in the `slurmctld` charm. This enables users to integrate with the file system of their choice, such as their own CephFS deployment, a cloud-specific managed file system, or another that meets latency requirements.

:::{warning}
The Slurm developers [do not recommended NFS](https://slurm.schedmd.com/quickstart_admin.html#Config) for the shared file system due to inadequate performance.
:::

The `slurmctld` charm automatically configures the mount point for the shared file system when integrated with the `filesystem-client` on the `mount` endpoint. The shared file system is mounted on all `slurmctld` units at `/srv/slurmctld-statefs`. The `StateSaveLocation` is then set to a sub-directory: `/srv/slurmctld-statefs/checkpoint`.

To allow for this automatic mount point configuration, the `filesystem-client` must be deployed without `--config mountpoint` set. Attempting to integrate a `filesystem-client` where `--config mountpoint` has been set will result in a charm error.

### Single `slurmctld` migration to high availability

In a non-HA setup (a single `slurmctld`), `StateSaveLocation` data is stored on the unit local disk at `/var/lib/slurm/checkpoint`. Before `slurmctld` backup units can be added to enable high availability, the `slurmctld` charm must be integrated with a `filesystem-client` on the `mount` endpoint to provide the necessary shared storage. On integration, the `StateSaveLocation` data is automatically copied from the local disk to the shared file system provided by the `filesystem-client`.

Once complete, `juju add-unit` can be used to add backup units. It is **not possible to remove the `filesystem-client` integration** and return to a non-HA setup once the migration has completed. To avoid data loss, the files and directories in the local  `/var/lib/slurm/checkpoint` are left untouched following migration. Specific steps can be found in the [Migrating a single `slurmctld` to high availability](howto-manage-single-slurmctld-to-high-availability) how-to section.

Note that **this migration requires cluster downtime**: the `slurmctld` service is stopped by the charm for the transfer duration and restarted when the `StateSaveLocation` data is in place on the shared file system. To minimize downtime, `StateSaveLocation` data is first copied to the shared file system while the `slurmctld` service is live, then the service is stopped and the difference in `StateSaveLocation` data is synchronized.

Be aware that attempting to scale up `slurmctld` without a `filesystem-client` will cause new units to enter `BlockedStatus` until the `filesystem-client` is integrated.

(explanation-slurmctld-high-availability-etc-slurm)=
### Shared `/etc/slurm` configuration data

In an HA setup, all `slurmctld` instances require consistent configuration files. That is, _slurm.conf_, _gres.conf_, and other Slurm configuration files must be identical on all `slurmctld` hosts. To achieve this in Charmed HPC, the shared file system enabled by the `filesystem-client` is used.

Similarly to `StateSaveLocation`, data in `/etc/slurm` is migrated to `/srv/slurmctld-statefs/etc/slurm` on `filesystem-client` integration. The `/etc/slurm` directory is then replaced with a symbolic link to `/srv/slurmctld-statefs/etc/slurm` on all `slurmctld` instances to ensure all access the same configuration files.

To avoid data loss, any existing `/etc/slurm/` is backed up to a date-stamped directory on the unit's local disk, for example `/etc/slurm_20250620_161437` for a backup performed on 2025-06-20 at 16:14:37. To prevent non-leaders from reading partially written configuration files, updates to files are made atomically via [slurmutils](https://github.com/charmed-hpc/slurmutils/).

The `slurmctld` charm leader in a Charmed HPC cluster handles all controller configuration operations. The leader generates cluster keys and all configuration files while non-leader units defer until these files appear in the shared storage.

Note that the charm leader under Juju and the primary `slurmctld` instance under Slurm may or may not be the same unit. Juju itself determines the charm leader while Slurm primary and backups are managed independently by the `slurmctld` charm in _slurm.conf_. Primary and backup order is determined by unit join order, with the most recent joining `slurmctld` instance being the lowest priority backup (the last `SlurmctldHost` entry in _slurm.conf_).
